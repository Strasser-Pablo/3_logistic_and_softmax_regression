{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic and Softmax Classifiers\n",
    "\n",
    "\n",
    "In this TP you have to implement two classifiers, the logistic who classify between two classes and the softmax who extends to many classes.\n",
    "\n",
    "You have the skeleton code and only need to write a few lines of code. What is important in this TP is not your code but your understanding of the problem. That's why we ask you to write and derive all the formulas on your report before implementing them. We will be vigilant regarding the correspondence of formulas and code.\n",
    "\n",
    "\n",
    "Here is a summary of what you will have to do :\n",
    "- implement a fully-vectorized **loss function**\n",
    "- **check your implementation** with the tests given\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n",
    "\n",
    "**LOOPS ARE NOT ALLOWED**. You must be able to write all the code you are asked for without loops. \n",
    "\n",
    "## Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_IRIS, load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make figures appear inline\n",
    "%matplotlib inline\n",
    "\n",
    "# notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load two classes of the Cifar10 dataset. We load only 2 classes because we start by implementing the logistic classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# load 2 classes\n",
    "cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "classes=['horse', 'car']\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir, classes=['horse', 'car'])\n",
    "\n",
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "print(\"Visualizing some samples\")\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# choising parameters for subsampling\n",
    "num_training = 9000\n",
    "num_validation = 1000\n",
    "\n",
    "# subsample the data\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "# Normalize the data: subtract the mean image and divide by the std\n",
    "mean_image = np.mean(X_train, axis = 0)\n",
    "std_image = np.std(X_train, axis = 0)\n",
    "X_train -= mean_image\n",
    "X_train /= std_image\n",
    "X_val -= mean_image\n",
    "X_val /= std_image\n",
    "X_test -= mean_image\n",
    "X_test /= std_image\n",
    "\n",
    "# add bias dimension and transform into columns\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "\n",
    "num_dim = X_train.shape[1]\n",
    "\n",
    "# Printing dimensions\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the computaion of scores ! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the logistic loss function.\n",
    "# Open the file logistic_regression.py and implement the\n",
    "# scores in loss method.\n",
    "from logistic_regression import Logistic\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "logistic_regression = Logistic(random_seed=13)\n",
    "logistic_regression.W = 0.001 * np.random.randn(dim, 1)\n",
    "scores = logistic_regression.loss(X_train)\n",
    "\n",
    "if scores is None:\n",
    "    print(\"You have to implement scores first.\")\n",
    "else:\n",
    "    if (np.sum(scores) - 4502.3308363) < 1e-7:\n",
    "        print(\"Great! Your implementation of scores seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of scores seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then implement the loss part of the loss function ! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the logistic loss function.\n",
    "# Open the file logistic_regression.py and implement the\n",
    "# loss in loss method.\n",
    "from logistic_regression import Logistic\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "logistic_regression = Logistic(random_seed=13)\n",
    "logistic_regression.W = 0.001 * np.random.randn(dim, 1)\n",
    "loss, _ = logistic_regression.loss(X_train, y_train, 0.0)\n",
    "\n",
    "if loss is None:\n",
    "    print(\"You have to implement loss first.\")\n",
    "else:\n",
    "    if np.abs(loss - 0.69346228) < 1e-7:\n",
    "        print(\"Great! Your implementation of the loss seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of  the loss seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the computation of the gradients in loss function ! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the logistic loss function.\n",
    "# Open the file logistic_regression.py and implement the\n",
    "# grad part in loss method.\n",
    "from logistic_regression import Logistic\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "logistic_regression = Logistic(random_seed=13)\n",
    "logistic_regression.W = 0.001 * np.random.randn(dim, 1)\n",
    "_, grad = logistic_regression.loss(X_train, y_train, 0.0)\n",
    "\n",
    "if not np.sum(grad):\n",
    "    print(\"You have to implement the gradients first.\")\n",
    "else:\n",
    "    if np.abs(np.sum(grad) - 28.4035320094) < 1e-7:\n",
    "        print(\"Great! Your implementation of gradients seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of gradients seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before start playing, we need to implement the prediction method of the classifier. Implement it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file logistic_regression.py and implement the\n",
    "# predict() method.\n",
    "from logistic_regression import Logistic\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "logistic_regression = Logistic(random_seed=13)\n",
    "logistic_regression.W = 0.001 * np.random.randn(dim, 1)\n",
    "y_pred = logistic_regression.predict(X_train)\n",
    "\n",
    "if not np.sum(y_pred):\n",
    "    print(\"You have to implement prediction first.\")\n",
    "else:\n",
    "    if np.abs(np.sum(y_pred) - 4718.0) < 1e-7:\n",
    "        print(\"Great! Your implementation of prediction seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of prediction seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use validation to tune the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths;\n",
    "from logistic_regression import Logistic\n",
    "import copy\n",
    "\n",
    "\n",
    "# to save loss of best model\n",
    "best_hist = []\n",
    "# to save accuracy on validation set of best model\n",
    "best_val = -1\n",
    "# to save best model\n",
    "best_logistic = None\n",
    "\n",
    "learning_rates = [1e-6, 1e-7, 1e-8]\n",
    "regularization_strengths = [1e-4, 1e-2, 0, 1e2, 1e4]\n",
    "# number of iterations to train\n",
    "num_iters=1500\n",
    "# if true display informations about training\n",
    "verbose = True\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        print(\"lr = {}, reg = {}\".format(lr, reg))\n",
    "        model = Logistic(random_seed=13)\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "        # set. For each combination of hyperparameters, train a model on the           #\n",
    "        # training set, compute its accuracy on the training and validation sets, and  #\n",
    "        # store the best validation accuracy in best_val and the model object that     #\n",
    "        # achieves this accuracy in best_logistc.                                      #\n",
    "        #                                                                              #\n",
    "        # Hint: You should use a small value for num_iters as you develop your         #\n",
    "        # validation code so that the model don't take much time to train; once you are#\n",
    "        # confident that your validation code works, you should rerun the validation   #\n",
    "        # code with a larger value for num_iters, lets say 1500.                       #\n",
    "        #                                                                              #\n",
    "        # To copy the model use best_logistic = copy.deepcopy(model)                   #\n",
    "        ################################################################################\n",
    "        pass\n",
    "    \n",
    "        acc_train = 0 # to replace\n",
    "        acc_val = 0 # to replace\n",
    "        ################################################################################\n",
    "        #                              END OF YOUR CODE                                #\n",
    "        ################################################################################\n",
    "        print(\"\\r\\t -> train acc = {:.3f}, val acc = {:.3f}\".format(acc_train, acc_val))\n",
    "\n",
    "\n",
    "print('best validation accuracy achieved during cross-validation: {:.3f}'.format(best_val))\n",
    "plt.plot(best_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the best model, we can test the accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_logistic.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('Logistic on raw pixels final test set accuracy: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally visualizy the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_logistic.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 1)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "\n",
    "# Rescale the weights to be between 0 and 255\n",
    "wimg = 255.0 * (w[:, :, :, 0].squeeze() - w_min) / (w_max - w_min)\n",
    "plt.imshow(wimg.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.title('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax clasifier\n",
    "\n",
    "We ask you to extend here your classifier to many classes. We reload again the data with all the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# load 2 classes\n",
    "cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "print(\"Visualizing some samples\")\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# choising parameters for subsampling\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "\n",
    "# subsample the data\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "# Normalize the data: subtract the mean image and divide by the std\n",
    "mean_image = np.mean(X_train, axis = 0)\n",
    "std_image = np.std(X_train, axis = 0)\n",
    "X_train -= mean_image\n",
    "#X_train /= std_image\n",
    "X_val -= mean_image\n",
    "#X_val /= std_image\n",
    "X_test -= mean_image\n",
    "#X_test /= std_image\n",
    "\n",
    "# add bias dimension and transform into columns\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "\n",
    "num_dim = X_train.shape[1]\n",
    "\n",
    "# Printing dimensions\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the computaion of scores ! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the logistic loss function.\n",
    "# Open the file logistic.py and implement the\n",
    "# scores in loss method.\n",
    "from softmax_classifier import Softmax\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "model = Softmax(random_seed=13)\n",
    "model.W = 0.001 * np.random.randn(dim, 10)\n",
    "scores = model.loss(X_train)\n",
    "\n",
    "if scores is None:\n",
    "    print(\"You have to implement scores first.\")\n",
    "else:\n",
    "    if (np.sum(scores[:, 0]) - 4897.34961844) < 1e-7:\n",
    "        print(\"Great! Your implementation of scores seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of scores seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then implement the loss part of the loss function ! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the logistic loss function.\n",
    "# Open the file logistic.py and implement the\n",
    "# loss in loss method.\n",
    "from softmax_classifier import Softmax\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "model = Softmax(random_seed=13)\n",
    "model.W = 0.001 * np.random.randn(dim, 10)\n",
    "loss, _ = model.loss(X_train, y_train, 0.0)\n",
    "\n",
    "if loss is None:\n",
    "    print(\"You have to implement loss first.\")\n",
    "else:\n",
    "    if np.abs(loss - 5.65453535) < 1e-7:\n",
    "        print(\"Great! Your implementation of the loss seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of  the loss seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the computation of the gradients in loss function ! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the logistic loss function.\n",
    "# Open the file logistic.py and implement the\n",
    "# grad part in loss method.\n",
    "from softmax_classifier import Softmax\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "model = Softmax(random_seed=13)\n",
    "model.W = 0.001 * np.random.randn(dim, 10)\n",
    "_, grad = model.loss(X_train, y_train, 0.0)\n",
    "\n",
    "if not np.sum(grad):\n",
    "    print(\"You have to implement the gradients first.\")\n",
    "else:\n",
    "    if np.abs(np.sum(grad)*1e15 + 8.881784197) < 1e-7:\n",
    "        print(\"Great! Your implementation of gradients seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of gradients seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before start playing, we need to implement the prediction method of the classifier. Implement it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file logistic_regression.py and implement the\n",
    "# predict() method.\n",
    "from softmax_classifier import Softmax\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "model = Softmax(random_seed=13)\n",
    "model.W = 0.001 * np.random.randn(dim, 10)\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "if not np.sum(y_pred):\n",
    "    print(\"You have to implement predictions first.\")\n",
    "else:\n",
    "    if np.abs(np.sum(y_pred) - 210313.) < 1e-7:\n",
    "        print(\"Great! Your implementation of predictions seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of predictions seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use validation to tune the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from softmax_classifier import Softmax\n",
    "import copy\n",
    "\n",
    "# to save loss of best model\n",
    "best_hist = []\n",
    "# to save accuracy on validation set of best model\n",
    "best_val = -1\n",
    "# to save best model\n",
    "best_logistic = None\n",
    "best_lr = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "learning_rates = [5e-7, 1e-6]\n",
    "regularization_strengths = [1e2, 1e3, 1e4, 1e5]\n",
    "# number of iterations to train\n",
    "num_iters = 1500\n",
    "# if true display informations about training\n",
    "verbose = True\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        print(\"lr = {}, reg = {}\".format(lr, reg))\n",
    "        model = Softmax(random_seed=13)\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "        # set. For each combination of hyperparameters, train a model on the           #\n",
    "        # training set, compute its accuracy on the training and validation sets, and  #\n",
    "        # store the best validation accuracy in best_val and the model object that     #\n",
    "        # achieves this accuracy in best_logistc.                                      #\n",
    "        #                                                                              #\n",
    "        # Hint: You should use a small value for num_iters as you develop your         #\n",
    "        # validation code so that the model don't take much time to train; once you are#\n",
    "        # confident that your validation code works, you should rerun the validation   #\n",
    "        # code with a larger value for num_iters, lets say 1500.                       #\n",
    "        #                                                                              #\n",
    "        # To copy the model use best_model = copy.deepcopy(model)                      #\n",
    "        ################################################################################\n",
    "        pass\n",
    "    \n",
    "        acc_train = 0 # to replace\n",
    "        acc_val = 0 # to replace\n",
    "        ################################################################################\n",
    "        #                              END OF YOUR CODE                                #\n",
    "        ################################################################################\n",
    "        print(\"\\r\\t -> train acc = {:.3f}, val acc = {:.3f}\".format(acc_train, acc_val))\n",
    "\n",
    "\n",
    "print('best validation accuracy achieved during cross-validation: {:.3f}'.format(best_val))\n",
    "plt.plot(best_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the best model, we can test the accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('Logistic on raw pixels final test set accuracy: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally visualizy the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_model.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
